"""
Pipeline de análisis de embeddings.

Este módulo proporciona un pipeline enfocado en el análisis y visualización
de embeddings, incluyendo reducción de dimensionalidad y clustering.
"""

from __future__ import annotations

import json
import logging
import tempfile
import shutil
import warnings
import zipfile
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional, Union

import numpy as np
import pandas as pd
import torch

from src.data.MyDataset import create_car_dataset
from src.utils.ClusteringAnalyzer import ClusteringAnalyzer
from src.utils.ClusterVisualizer import ClusterVisualizer
from src.utils.DimensionalityReducer import DimensionalityReducer
from src.utils.JsonUtils import safe_json_dump


# Configuración de warnings y logging
warnings.filterwarnings('ignore')
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


class EmbeddingsPipelineError(Exception):
    """Excepción personalizada para errores del pipeline de embeddings."""
    pass


class EmbeddingsPipeline:
    """
    Pipeline para análisis de embeddings de modelos de visión.
    
    Esta clase maneja el análisis completo de embeddings: extracción,
    reducción de dimensionalidad, clustering y visualización.
    
    Cambios principales:
    - Dataset con splits porcentuales (train_ratio, val_ratio, test_ratio)
    - Usa 'standard' sampling strategy para análisis de embeddings
    - Compatible con embeddings de modelos fine-tuned con pytorch-metric-learning
    
    Example:
        >>> config = {
        ...     'min_images': 5,
        ...     'train_ratio': 0.7,
        ...     'val_ratio': 0.2,
        ...     'test_ratio': 0.1,
        ...     'views': ['front'],
        ...     'seed': 3
        ... }
        >>> pipeline = EmbeddingsPipeline(config, dataframe)
        >>> pipeline.create_dataset_for_labels()
        >>> pipeline.load_embeddings_from_files('baseline.pt')
        >>> results = pipeline.run_full_pipeline()
    """
    
    def __init__(
        self, 
        config: Dict[str, Any], 
        df: pd.DataFrame,
        experiment_name: Optional[str] = None
    ) -> None:
        self.config = config
        self.df = df
        self.experiment_name = experiment_name or f"embeddings_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Componentes del pipeline
        self.dataset_dict = None
        self.baseline_embeddings = None
        self.baseline_labels = None
        self.finetuned_embeddings = None
        
        # Información de los embeddings cargados
        self.baseline_info = None
        self.finetuned_info = None
        
        self.results = {
            'config': config.copy(),
            'experiment_name': self.experiment_name,
            'start_time': datetime.now().isoformat()
        }
        
        logging.info(f"Inicializado EmbeddingsPipeline: {self.experiment_name}")
    
    def create_dataset_for_labels(self) -> None:
        """
        Crea el dataset solo para obtener las etiquetas correctas.
        
        Usa los nuevos parámetros porcentuales para crear el dataset.
        """
        logging.info("Creando dataset para obtener etiquetas...")
        
        self.dataset_dict = create_car_dataset(
            df=self.df,
            views=self.config.get('views', ['front']),
            min_images=self.config.get('min_images', 5),
            train_ratio=self.config.get('train_ratio', 0.7),
            val_ratio=self.config.get('val_ratio', 0.2),
            test_ratio=self.config.get('test_ratio', 0.1),
            batch_size=self.config.get('batch_size', 32),
            num_workers=self.config.get('num_workers', 4),
            seed=self.config.get('seed', 3),
            sampling_strategy='standard',
            model_type='vision',
            description_include=''
        )

        self.dataset_dict['dataset'].set_split('val')

        labels = []
        for sample_tuple in self.dataset_dict['dataset'].val_samples:
            model_year_tuple = sample_tuple[0]
            label_str = f"{model_year_tuple[0]}_{model_year_tuple[1]}"
            label = self.dataset_dict['dataset'].label_encoder.transform([label_str])[0]
            labels.append(label)
        
        self.baseline_labels = torch.tensor(labels)
        
        self.results['dataset_info'] = {
            'num_training_classes': self.dataset_dict['dataset'].get_num_classes_for_training(),
            'num_total_classes': self.dataset_dict['dataset'].get_total_num_classes(),
            'val_samples': len(self.dataset_dict['dataset'].val_samples),
            'views': self.dataset_dict['dataset'].views,
            'num_regular_classes': len(self.dataset_dict['dataset'].regular_classes),
            'num_oneshot_classes': len(self.dataset_dict['dataset'].oneshot_classes)
        }
        
        logging.info(f"Dataset creado - val samples: {len(self.dataset_dict['dataset'].val_samples)}")
        logging.info(f"Etiquetas extraídas: {len(self.baseline_labels)} muestras")
    
    def load_embeddings_from_files(
        self, 
        baseline_embeddings_path: Union[str, Path],
        finetuned_embeddings_path: Optional[Union[str, Path]] = None
    ) -> None:
        """
        Carga embeddings desde archivos especificados.
        
        Args:
            baseline_embeddings_path: Ruta al archivo de embeddings baseline
            finetuned_embeddings_path: Ruta al archivo de embeddings fine-tuned (opcional)
        """
        logging.info("Cargando embeddings desde archivos...")
        # Cargar embeddings baseline
        baseline_path = Path(baseline_embeddings_path)
        if not baseline_path.exists():
            raise EmbeddingsPipelineError(f"Archivo de embeddings baseline no encontrado: {baseline_path}")
        
        self.baseline_embeddings = torch.load(baseline_path, map_location='cpu')
        self.baseline_info = {
            'path': str(baseline_path),
            'shape': list(self.baseline_embeddings.shape),
            'device': str(self.baseline_embeddings.device)
        }
        
        logging.info(f"Embeddings baseline cargados: {self.baseline_embeddings.shape}")
        # Cargar embeddings fine-tuned si se proporciona
        if finetuned_embeddings_path:
            finetuned_path = Path(finetuned_embeddings_path)
            if not finetuned_path.exists():
                raise EmbeddingsPipelineError(f"Archivo de embeddings fine-tuned no encontrado: {finetuned_path}")
            
            self.finetuned_embeddings = torch.load(finetuned_path, map_location='cpu')
            self.finetuned_info = {
                'path': str(finetuned_path),
                'shape': list(self.finetuned_embeddings.shape),
                'device': str(self.finetuned_embeddings.device)
            }
            
            logging.info(f"Embeddings fine-tuned cargados: {self.finetuned_embeddings.shape}")
            
            if self.baseline_embeddings.shape != self.finetuned_embeddings.shape:
                raise EmbeddingsPipelineError(
                    f"Las formas de embeddings no coinciden: "
                    f"baseline {self.baseline_embeddings.shape} vs "
                    f"finetuned {self.finetuned_embeddings.shape}"
                )
        
        self.results['embeddings_info'] = {
            'baseline': self.baseline_info,
            'finetuned': self.finetuned_info if finetuned_embeddings_path else None
        }
    
    def load_embeddings_from_zip(
        self,
        zip_path: Union[str, Path]
    ) -> None:
        """
        Carga embeddings desde un archivo ZIP.

        Args:
            zip_path: Ruta al archivo ZIP que contiene los embeddings y configuración
        """
        logging.info(f"Cargando embeddings desde ZIP: {zip_path}")
        
        # Extraer y cargar archivos del ZIP
        zip_path = Path(zip_path)
        if not zip_path.exists():
            raise EmbeddingsPipelineError(f"Archivo ZIP no encontrado: {zip_path}")
        
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(temp_path)
            
            baseline_files = list(temp_path.rglob("baseline_embeddings.pt"))
            finetuned_files = list(temp_path.rglob("finetuned_embeddings.pt"))
            
            if not baseline_files:
                raise EmbeddingsPipelineError("No se encontró baseline_embeddings.pt en el ZIP")
            
            baseline_path = baseline_files[0]
            finetuned_path = finetuned_files[0] if finetuned_files else None
            
            self.load_embeddings_from_files(baseline_path, finetuned_path)
            
            config_files = list(temp_path.rglob("config.json"))
            if config_files:
                with open(config_files[0], 'r') as f:
                    loaded_config = json.load(f)
                    self.results['original_config'] = loaded_config
                    logging.info("Configuración original cargada desde ZIP")
    
    def analyze_baseline_embeddings(self) -> Dict[str, Any]:
        """
        Analiza los embeddings baseline cargados.

        Returns:
            Diccionario con resultados del análisis
        """
        if self.baseline_embeddings is None:
            raise EmbeddingsPipelineError("Embeddings baseline no cargados. Ejecutar load_embeddings_from_files() primero.")
        
        if self.baseline_labels is None:
            raise EmbeddingsPipelineError("Etiquetas no disponibles. Ejecutar create_dataset_for_labels() primero.")
        
        logging.info("Analizando embeddings baseline cargados...")

        # Realizar análisis
        baseline_results = self._analyze_embeddings(
            embeddings=self.baseline_embeddings,
            labels=self.baseline_labels,
            phase='baseline'
        )
        
        # Guardar resultados
        self.results['baseline_analysis'] = baseline_results
        
        logging.info("Análisis baseline completado!")
        return baseline_results
    
    def analyze_finetuned_embeddings(self) -> Dict[str, Any]:
        """
        Analiza los embeddings fine-tuned cargados.

        Returns:
            Diccionario con resultados del análisis
        """
        if self.finetuned_embeddings is None:
            raise EmbeddingsPipelineError("Embeddings fine-tuned no cargados. Deben cargarse con load_embeddings_from_files().")
        
        if self.baseline_labels is None:
            raise EmbeddingsPipelineError("Etiquetas no disponibles. Ejecutar create_dataset_for_labels() primero.")
        
        logging.info("Analizando embeddings fine-tuneados cargados...")

        # Realizar análisis
        finetuned_results = self._analyze_embeddings(
            embeddings=self.finetuned_embeddings,
            labels=self.baseline_labels,
            phase='finetuned'
        )

        # Guardar resultados
        self.results['finetuned_analysis'] = finetuned_results
        
        logging.info("Análisis fine-tuneado completado!")
        return finetuned_results
    
    def _analyze_embeddings(
        self, 
        embeddings: torch.Tensor, 
        labels: torch.Tensor, 
        phase: str
    ) -> Dict[str, Any]:
        """
        Realiza el análisis completo de reducción de dimensionalidad,
        clustering y visualización para un conjunto de embeddings dado.

        Args:
            embeddings: Tensor de embeddings a analizar.
            labels: Tensor de etiquetas verdaderas correspondientes a los embeddings.
            phase: Fase del análisis ('baseline' o 'finetuned').

        Returns:
            Diccionario con resultados del análisis.
        """
        results = {}
        
        logging.info(f"Ejecutando reducción de dimensionalidad - {phase}...")
        
        # Reducción de dimensionalidad
        reducer = DimensionalityReducer(
            embeddings=embeddings,
            labels=labels,
            seed=self.config.get('seed', 3),
            optimizer_trials=self.config.get('reducer_trials', 50),
            available_methods=self.config.get('reducer_methods', ['pca', 'umap']),
            n_jobs=self.config.get('n_jobs', -1),
            use_incremental=self.config.get('use_incremental', True)
        )
        
        # Ejecutar reducción de dimensionalidad
        reduction_scores = reducer.reduce_all()
        best_method, best_embeddings = reducer.get_best_result()
        
        # Guardar resultados de reducción
        results['reduction'] = {
            'scores': reduction_scores,
            'best_method': best_method,
            'best_embeddings_shape': list(best_embeddings.shape)
        }
        
        logging.info(f"Ejecutando clustering - {phase}...")
        
        # Clustering
        clustering = ClusteringAnalyzer(
            embeddings=best_embeddings,
            true_labels=labels,
            seed=self.config.get('seed', 3),
            optimizer_trials=self.config.get('clustering_trials', 50),
            available_methods=self.config.get('clustering_methods', ['dbscan', 'hdbscan']),
            n_jobs=self.config.get('n_jobs', -1)
        )
        
        # Ejecutar clustering
        clustering_results = clustering.cluster_all()
        logging.info(f"Clustering results keys: {list(clustering_results.keys()) if clustering_results else 'Empty results'}")
        
        if not clustering_results:
            raise ValueError(f"No se pudieron generar resultados de clustering para {phase}")
        
        # Obtener mejor método de clustering
        comparison_df = clustering.compare_methods()
        best_clustering_method, best_cluster_labels = clustering.get_best_result()
        
        # Guardar resultados de clustering
        results['clustering'] = {
            'results': clustering_results,
            'comparison_df': comparison_df.to_dict('records'),
            'best_method': best_clustering_method
        }
        
        if self.config.get('generate_visualizations', True):
            logging.info(f"Generando visualizaciones - {phase}...")
            
            visualizer = ClusterVisualizer(
                embeddings=best_embeddings,
                cluster_labels=best_cluster_labels,
                true_labels=labels,
                val_samples=self.dataset_dict['dataset'].val_samples,
                label_encoder=self.dataset_dict['dataset'].label_encoder,
                seed=self.config.get('seed', 3)
            )
            
            # Guardar visualizador para usar en save_results()
            if phase == 'baseline':
                self.baseline_visualizer = visualizer
            elif phase == 'finetuned':
                self.finetuned_visualizer = visualizer
            
            # Estadísticas y resumen
            visualizer.print_cluster_statistics()
            summary_df = clustering.get_cluster_summary(best_clustering_method)

            # Número de clusters a visualizar 
            n_to_vis = int(self.config.get('n_clusters_to_visualize', 3))
            max_classes_per_cluster = int(self.config.get('max_classes_per_cluster_viz', 8))

            # Usar estrategia adaptativa de visualización
            try:
                visualizer.visualize_best_available_clusters(n_to_vis, max_classes_per_cluster)
            except Exception as e:
                logging.error(f"Error en visualización adaptativa de clusters: {e}")
                # Fallback: intentar visualizaciones individuales
                try:
                    logging.info("Intentando visualización de clusters puros como fallback...")
                    visualizer.visualize_good_clusters(n_to_vis, max_classes_per_cluster)
                except Exception as e2:
                    logging.warning(f"Error visualizando clusters puros: {e2}")

                try:
                    logging.info("Intentando visualización de clusters mixtos como fallback...")
                    visualizer.visualize_mixed_clusters(n_to_vis, max_classes_per_cluster)
                except Exception as e3:
                    logging.warning(f"Error visualizando clusters mixtos: {e3}")

            # Obtener solapamiento de clases entre clusters
            try:
                overlap_df = visualizer.get_class_cluster_overlap()
                overlap_records = overlap_df.to_dict('records') if isinstance(overlap_df, pd.DataFrame) else []
            except Exception as e:
                logging.warning(f"Error calculando solapamiento de clases: {e}")
                overlap_records = []

            # Obtener overlap de clases
            try:
                overlap_df = visualizer.get_class_cluster_overlap()
                overlap_records = overlap_df.to_dict('records') if not overlap_df.empty else []
            except Exception as e:
                logging.warning(f"No se pudo generar overlap de clases: {e}")
                overlap_records = []
            
            results['visualization'] = {
                'cluster_summary': summary_df.to_dict('records'),
                'cluster_analysis': visualizer.cluster_analysis,
                'class_cluster_overlap': overlap_records
            }
        
        logging.info(f"Análisis {phase} - Mejor reducción: {best_method}, Mejor clustering: {best_clustering_method}")
        
        return results
    
    def compare_results(self) -> Dict[str, Any]:
        """
        Compara los resultados entre embeddings baseline y fine-tuned.

        Returns:
            Diccionario con resultados de la comparación
        """
        if 'baseline_analysis' not in self.results or 'finetuned_analysis' not in self.results:
            raise EmbeddingsPipelineError("Ambos análisis (baseline y finetuned) deben completarse primero.")
        
        logging.info("Comparando resultados baseline vs fine-tuned...")
        
        # Extraer resultados relevantes
        baseline = self.results['baseline_analysis']
        finetuned = self.results['finetuned_analysis']
        
        # Comparar clustering
        baseline_clustering = baseline['clustering']['comparison_df']
        finetuned_clustering = finetuned['clustering']['comparison_df']
        
        # Construir comparación
        comparison = {
            'reduction_methods': {
                'baseline_best': baseline['reduction']['best_method'],
                'finetuned_best': finetuned['reduction']['best_method']
            },
            'clustering_methods': {
                'baseline_best': baseline['clustering']['best_method'],
                'finetuned_best': finetuned['clustering']['best_method']
            },
            'clustering_metrics_comparison': self._compare_clustering_metrics(
                baseline_clustering, finetuned_clustering
            )
        }
        
        # Calcular mejora de rendimiento basada en ARI
        if  baseline_clustering and finetuned_clustering:
            baseline_best = max(baseline_clustering, key=lambda x: x.get('adjusted_rand_score', 0))
            finetuned_best = max(finetuned_clustering, key=lambda x: x.get('adjusted_rand_score', 0))
            
            comparison['performance_improvement'] = {
                'baseline_ari': baseline_best.get('adjusted_rand_score', 0),
                'finetuned_ari': finetuned_best.get('adjusted_rand_score', 0),
                'ari_improvement': finetuned_best.get('adjusted_rand_score', 0) - baseline_best.get('adjusted_rand_score', 0)
            }
        
        # Guardar comparación en resultados
        self.results['comparison'] = comparison
        
        logging.info("Comparación completada!")
        logging.info(f"Mejor método baseline: {comparison['clustering_methods']['baseline_best']}")
        logging.info(f"Mejor método fine-tuned: {comparison['clustering_methods']['finetuned_best']}")
        
        return comparison
    
    def _compare_clustering_metrics(
        self, 
        baseline_results: list, 
        finetuned_results: list
    ) -> Dict[str, Any]:
        """
        Compara las métricas de clustering entre los resultados baseline y fine-tuned.

        Args:
            baseline_results (list): Resultados de clustering baseline.
            finetuned_results (list): Resultados de clustering fine-tuned.

        Returns:
            Dict[str, Any]: Diccionario con la comparación de métricas.
        """
        if not baseline_results or not finetuned_results:
            return {}
        
        # Comparar métricas clave
        metrics = ['adjusted_rand_score', 'silhouette_score', 'calinski_harabasz_score']
        comparison = {}
        
        # Calcular estadísticas para cada métrica
        for metric in metrics:
            baseline_values = [r.get(metric, 0) for r in baseline_results]
            finetuned_values = [r.get(metric, 0) for r in finetuned_results]
            
            if baseline_values and finetuned_values:
                comparison[metric] = {
                    'baseline_max': max(baseline_values),
                    'baseline_mean': np.mean(baseline_values),
                    'finetuned_max': max(finetuned_values),
                    'finetuned_mean': np.mean(finetuned_values),
                    'improvement_max': max(finetuned_values) - max(baseline_values),
                    'improvement_mean': np.mean(finetuned_values) - np.mean(baseline_values)
                }
        
        return comparison
    
    def save_results(self, save_dir: Union[str, Path] = "results") -> Path:
        """
        Guarda todos los resultados del pipeline en un directorio especificado.

        Args:
            save_dir (Union[str, Path]): Directorio donde se guardarán los resultados.
        Returns:
            Path: Ruta al archivo zip que contiene los resultados guardados.
        """
        # Crear directorio de resultados
        save_dir = Path(save_dir)
        experiment_dir = save_dir / self.experiment_name
        experiment_dir.mkdir(parents=True, exist_ok=True)
        
        logging.info(f"Guardando resultados en: {experiment_dir}")
        
        # Guardar configuración
        safe_json_dump(self.config, experiment_dir / "config.json")
        
        # Guardar resultados
        self.results['end_time'] = datetime.now().isoformat()
        results_to_save = self.results.copy()
        
        safe_json_dump(results_to_save, experiment_dir / "results.json")
        
        if self.baseline_embeddings is not None:
            np.save(experiment_dir / "baseline_embeddings.npy", self.baseline_embeddings.numpy())
        
        if self.finetuned_embeddings is not None:
            np.save(experiment_dir / "finetuned_embeddings.npy", self.finetuned_embeddings.numpy())
        
        if self.baseline_labels is not None:
            np.save(experiment_dir / "labels.npy", self.baseline_labels.numpy())
        
        if 'comparison' in self.results:
            comparison_df = pd.DataFrame([self.results['comparison']['performance_improvement']])
            comparison_df.to_csv(experiment_dir / "performance_comparison.csv", index=False)
        
        # Guardar visualizaciones de baseline si están disponibles
        if hasattr(self, 'baseline_visualizer') and self.baseline_visualizer is not None:
            try:
                visualizations_dir = experiment_dir / "visualizations" / "baseline"
                logging.info("Guardando visualizaciones baseline...")
                saved_viz_files = self.baseline_visualizer.save_visualizations(
                    visualizations_dir,
                    n_pure_clusters=5,
                    n_mixed_clusters=5,
                    n_images_per_cluster=6
                )
                logging.info(f"Visualizaciones baseline guardadas: {list(saved_viz_files.keys())}")
            except Exception as e:
                logging.warning(f"No se pudieron guardar las visualizaciones baseline: {e}")
        
        # Guardar visualizaciones de finetuned si están disponibles
        if hasattr(self, 'finetuned_visualizer') and self.finetuned_visualizer is not None:
            try:
                visualizations_dir = experiment_dir / "visualizations" / "finetuned"
                logging.info("Guardando visualizaciones finetuned...")
                saved_viz_files = self.finetuned_visualizer.save_visualizations(
                    visualizations_dir,
                    n_pure_clusters=5,
                    n_mixed_clusters=5,
                    n_images_per_cluster=6
                )
                logging.info(f"Visualizaciones finetuned guardadas: {list(saved_viz_files.keys())}")
            except Exception as e:
                logging.warning(f"No se pudieron guardar las visualizaciones finetuned: {e}")
        
        # Crear archivo ZIP con todos los resultados
        zip_path = save_dir / f"{self.experiment_name}.zip"
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file_path in experiment_dir.rglob("*"):
                if file_path.is_file():
                    arcname = file_path.relative_to(save_dir)
                    zipf.write(file_path, arcname)
        
        if experiment_dir.exists():
            shutil.rmtree(experiment_dir)
        
        logging.info(f"Resultados guardados en ZIP: {zip_path}")
        return zip_path
    
    def run_baseline_analysis_from_zip(self, zip_path: Union[str, Path]) -> Dict[str, Any]:
        """
        Ejecuta el análisis baseline completo desde un archivo ZIP.
        Args:
            zip_path: Ruta al archivo ZIP que contiene los embeddings y configuración
        Returns:
            Dict[str, Any]: Resultados del análisis baseline.
        """
        logging.info(f"=== ANÁLISIS BASELINE DESDE ZIP: {self.experiment_name} ===")
        
        try:
            self.create_dataset_for_labels()
            self.load_embeddings_from_zip(zip_path)
            self.analyze_baseline_embeddings()
            
            zip_result_path = self.save_results()
            self.results['saved_to'] = str(zip_result_path)
            
            logging.info("=== ANÁLISIS BASELINE COMPLETADO ===")
            logging.info(f"Resultados guardados en: {zip_result_path}")
            return self.results
            
        except Exception as e:
            logging.error(f"Error en análisis baseline: {e}")
            self.results['error'] = str(e)
            raise EmbeddingsPipelineError(f"Error en análisis baseline: {e}") from e
    
    def run_finetuned_analysis_from_zip(self, zip_path: Union[str, Path]) -> Dict[str, Any]:
        """
        Ejecuta el análisis fine-tuned completo desde un archivo ZIP.

        Args:
            zip_path: Ruta al archivo ZIP que contiene los embeddings y configuración

        Returns:
            Dict[str, Any]: Resultados del análisis fine-tuned.
        """
        logging.info(f"=== ANÁLISIS FINETUNED DESDE ZIP: {self.experiment_name} ===")
        
        try:
            if self.baseline_labels is None:
                self.create_dataset_for_labels()
            
            self.load_embeddings_from_zip(zip_path)
            
            if self.finetuned_embeddings is None:
                raise EmbeddingsPipelineError("No se encontraron embeddings finetuned en el ZIP")
            
            self.analyze_finetuned_embeddings()
            
            zip_result_path = self.save_results()
            self.results['saved_to'] = str(zip_result_path)
            
            logging.info("=== ANÁLISIS FINETUNED COMPLETADO ===")
            logging.info(f"Resultados guardados en: {zip_result_path}")
            return self.results
            
        except Exception as e:
            logging.error(f"Error en análisis finetuned: {e}")
            self.results['error'] = str(e)
            raise EmbeddingsPipelineError(f"Error en análisis finetuned: {e}") from e
    
    def run_comparative_analysis_from_zip(self, zip_path: Union[str, Path]) -> Dict[str, Any]:
        """
        Ejecuta el análisis comparativo completo desde un archivo ZIP.

        Args:
            zip_path: Ruta al archivo ZIP que contiene los embeddings y configuración

        Returns:
            Dict[str, Any]: Resultados del análisis comparativo.
        """
        logging.info(f"=== ANÁLISIS COMPARATIVO DESDE ZIP: {self.experiment_name} ===")
        
        try:
            self.create_dataset_for_labels()
            self.load_embeddings_from_zip(zip_path)
            
            if self.finetuned_embeddings is None:
                raise EmbeddingsPipelineError("Se requieren embeddings finetuned para comparación")
            
            self.analyze_baseline_embeddings()
            self.analyze_finetuned_embeddings()
            self.compare_results()
            
            zip_result_path = self.save_results()
            self.results['saved_to'] = str(zip_result_path)
            
            logging.info("=== ANÁLISIS COMPARATIVO COMPLETADO ===")
            logging.info(f"Resultados guardados en: {zip_result_path}")
            return self.results
            
        except Exception as e:
            logging.error(f"Error en análisis comparativo: {e}")
            self.results['error'] = str(e)
            raise EmbeddingsPipelineError(f"Error en análisis comparativo: {e}") from e
    
    def run_full_analysis_from_zip(self, zip_path: Union[str, Path]) -> Dict[str, Any]:
        """
        Ejecuta el análisis completo (baseline o comparativo) desde un archivo ZIP.

        Args:
            zip_path: Ruta al archivo ZIP que contiene los embeddings y configuración

        Returns:
            Dict[str, Any]: Resultados del análisis completo.
        """
        return self.run_comparative_analysis_from_zip(zip_path)
    
    def run_baseline_analysis_from_files(self, baseline_embeddings_path: Union[str, Path]) -> Dict[str, Any]:
        """
        Ejecuta el análisis baseline completo desde archivos de embeddings.
        
        Args:
            baseline_embeddings_path: Ruta al archivo de embeddings baseline

        Returns:
            Dict[str, Any]: Resultados del análisis baseline.
        """
        logging.info(f"=== ANÁLISIS BASELINE DESDE ARCHIVOS: {self.experiment_name} ===")
        
        try:
            self.create_dataset_for_labels()
            self.load_embeddings_from_files(baseline_embeddings_path, None)
            self.analyze_baseline_embeddings()
            
            zip_result_path = self.save_results()
            self.results['saved_to'] = str(zip_result_path)
            
            logging.info("=== ANÁLISIS BASELINE COMPLETADO ===")
            logging.info(f"Resultados guardados en: {zip_result_path}")
            return self.results
            
        except Exception as e:
            logging.error(f"Error en análisis baseline: {e}")
            self.results['error'] = str(e)
            raise EmbeddingsPipelineError(f"Error en análisis baseline: {e}") from e
    
    def run_finetuned_analysis_from_files(self, finetuned_embeddings_path: Union[str, Path]) -> Dict[str, Any]:
        """
        Ejecuta el análisis fine-tuned completo desde archivos de embeddings.

        Args:
            finetuned_embeddings_path: Ruta al archivo de embeddings fine-tuned

        Returns:
            Dict[str, Any]: Resultados del análisis fine-tuned.
        """
        logging.info(f"=== ANÁLISIS FINETUNED DESDE ARCHIVOS: {self.experiment_name} ===")
        
        try:
            if self.baseline_labels is None:
                self.create_dataset_for_labels()
            
            baseline_path = Path(finetuned_embeddings_path).parent / "baseline_embeddings.pt"
            if not baseline_path.exists():
                raise EmbeddingsPipelineError(f"Embeddings baseline no encontrados en {baseline_path}")
            
            self.load_embeddings_from_files(baseline_path, finetuned_embeddings_path)
            self.analyze_finetuned_embeddings()
            
            zip_result_path = self.save_results()
            self.results['saved_to'] = str(zip_result_path)
            
            logging.info("=== ANÁLISIS FINETUNED COMPLETADO ===")
            logging.info(f"Resultados guardados en: {zip_result_path}")
            return self.results
            
        except Exception as e:
            logging.error(f"Error en análisis finetuned: {e}")
            self.results['error'] = str(e)
            raise EmbeddingsPipelineError(f"Error en análisis finetuned: {e}") from e
    
    def run_comparative_analysis_from_files(
        self,
        baseline_embeddings_path: Union[str, Path],
        finetuned_embeddings_path: Union[str, Path]
    ) -> Dict[str, Any]:
        """
        Ejecuta el análisis comparativo completo desde archivos de embeddings.
        
        Args:
            baseline_embeddings_path: Ruta al archivo de embeddings baseline
            finetuned_embeddings_path: Ruta al archivo de embeddings fine-tuned
        Returns:
            Dict[str, Any]: Resultados del análisis comparativo.
        """
        logging.info(f"=== ANÁLISIS COMPARATIVO DESDE ARCHIVOS: {self.experiment_name} ===")
        
        try:
            self.create_dataset_for_labels()
            self.load_embeddings_from_files(baseline_embeddings_path, finetuned_embeddings_path)
            self.analyze_baseline_embeddings()
            self.analyze_finetuned_embeddings()
            self.compare_results()
            
            zip_result_path = self.save_results()
            self.results['saved_to'] = str(zip_result_path)
            
            logging.info("=== ANÁLISIS COMPARATIVO COMPLETADO ===")
            logging.info(f"Resultados guardados en: {zip_result_path}")
            return self.results
            
        except Exception as e:
            logging.error(f"Error en análisis comparativo: {e}")
            self.results['error'] = str(e)
            raise EmbeddingsPipelineError(f"Error en análisis comparativo: {e}") from e
    
    def run_full_analysis_from_files(
        self,
        baseline_embeddings_path: Union[str, Path],
        finetuned_embeddings_path: Optional[Union[str, Path]] = None
    ) -> Dict[str, Any]:
        """
        Ejecuta el análisis completo (baseline o comparativo) desde archivos de embeddings.
        
        Args:
            baseline_embeddings_path: Ruta al archivo de embeddings baseline
            finetuned_embeddings_path: Ruta al archivo de embeddings fine-tuned (opcional)

        Returns:
            Dict[str, Any]: Resultados del análisis completo.
        """
        if finetuned_embeddings_path is None:
            return self.run_baseline_analysis_from_files(baseline_embeddings_path)
        else:
            return self.run_comparative_analysis_from_files(baseline_embeddings_path, finetuned_embeddings_path)
    
    def load_results_from_zip(self, zip_path: Union[str, Path]) -> Dict[str, Any]:
        """
        Carga los resultados guardados desde un archivo ZIP.

        Args:
            zip_path: Ruta al archivo ZIP que contiene los resultados guardados

        Returns:
            Dict[str, Any]: Resultados cargados desde el ZIP.
        """
        logging.info(f"Cargando resultados desde ZIP: {zip_path}")
        
        zip_path = Path(zip_path)
        if not zip_path.exists():
            raise EmbeddingsPipelineError(f"Archivo ZIP no encontrado: {zip_path}")
        
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(temp_path)
            
            results_files = list(temp_path.rglob("results.json"))
            
            if not results_files:
                raise EmbeddingsPipelineError("No se encontró results.json en el ZIP")
            
            with open(results_files[0], 'r') as f:
                loaded_results = json.load(f)
            
            logging.info(f"Resultados cargados correctamente desde: {zip_path}")
            return loaded_results
    
    def compare_saved_results(
        self,
        baseline_zip_path: Union[str, Path],
        finetuned_zip_path: Union[str, Path],
        save_comparison: bool = True
    ) -> Dict[str, Any]:
        """
        Compara los resultados guardados entre dos archivos ZIP.

        Args:
            baseline_zip_path: Ruta al archivo ZIP con resultados baseline
            finetuned_zip_path: Ruta al archivo ZIP con resultados fine-tuned
            save_comparison: Indica si se debe guardar la comparación en archivos JSON y CSV

        Returns:
            Dict[str, Any]: Resultados de la comparación.
        """
        logging.info("=== COMPARACIÓN DE RESULTADOS GUARDADOS ===")
        logging.info(f"Baseline: {baseline_zip_path}")
        logging.info(f"Finetuned: {finetuned_zip_path}")
        
        try:
            baseline_results = self.load_results_from_zip(baseline_zip_path)
            finetuned_results = self.load_results_from_zip(finetuned_zip_path)
            
            if 'baseline_analysis' not in baseline_results:
                raise EmbeddingsPipelineError("El ZIP baseline no contiene análisis baseline")
            
            if 'finetuned_analysis' not in finetuned_results:
                raise EmbeddingsPipelineError("El ZIP finetuned no contiene análisis finetuned")
            
            baseline_analysis = baseline_results['baseline_analysis']
            finetuned_analysis = finetuned_results['finetuned_analysis']
            
            comparison = {
                'baseline_zip': str(baseline_zip_path),
                'finetuned_zip': str(finetuned_zip_path),
                'comparison_timestamp': datetime.now().isoformat(),
                'reduction_methods': {
                    'baseline_best': baseline_analysis['reduction']['best_method'],
                    'finetuned_best': finetuned_analysis['reduction']['best_method']
                },
                'clustering_methods': {
                    'baseline_best': baseline_analysis['clustering']['best_method'],
                    'finetuned_best': finetuned_analysis['clustering']['best_method']
                }
            }
            
            baseline_clustering = baseline_analysis['clustering']['comparison_df']
            finetuned_clustering = finetuned_analysis['clustering']['comparison_df']
            
            comparison['clustering_metrics_comparison'] = self._compare_clustering_metrics(
                baseline_clustering, finetuned_clustering
            )
            
            if baseline_clustering and finetuned_clustering:
                baseline_best = max(baseline_clustering, key=lambda x: x.get('adjusted_rand_score', 0))
                finetuned_best = max(finetuned_clustering, key=lambda x: x.get('adjusted_rand_score', 0))
                
                comparison['performance_improvement'] = {
                    'baseline_ari': baseline_best.get('adjusted_rand_score', 0),
                    'finetuned_ari': finetuned_best.get('adjusted_rand_score', 0),
                    'ari_improvement': finetuned_best.get('adjusted_rand_score', 0) - baseline_best.get('adjusted_rand_score', 0),
                    'baseline_silhouette': baseline_best.get('silhouette_score', 0),
                    'finetuned_silhouette': finetuned_best.get('silhouette_score', 0),
                    'silhouette_improvement': finetuned_best.get('silhouette_score', 0) - baseline_best.get('silhouette_score', 0),
                    'baseline_nmi': baseline_best.get('normalized_mutual_info', 0),
                    'finetuned_nmi': finetuned_best.get('normalized_mutual_info', 0),
                    'nmi_improvement': finetuned_best.get('normalized_mutual_info', 0) - baseline_best.get('normalized_mutual_info', 0)
                }
            
            logging.info("\n" + "="*80)
            logging.info("RESUMEN DE COMPARACIÓN")
            logging.info("="*80)
            logging.info(f"Mejor método baseline: {comparison['clustering_methods']['baseline_best']}")
            logging.info(f"Mejor método finetuned: {comparison['clustering_methods']['finetuned_best']}")
            
            if 'performance_improvement' in comparison:
                perf = comparison['performance_improvement']
                logging.info("\nMEJORAS EN RENDIMIENTO:")
                logging.info(f"  ARI: {perf['baseline_ari']:.4f} → {perf['finetuned_ari']:.4f} (Δ {perf['ari_improvement']:+.4f})")
                logging.info(f"  Silhouette: {perf['baseline_silhouette']:.4f} → {perf['finetuned_silhouette']:.4f} (Δ {perf['silhouette_improvement']:+.4f})")
                logging.info(f"  NMI: {perf['baseline_nmi']:.4f} → {perf['finetuned_nmi']:.4f} (Δ {perf['nmi_improvement']:+.4f})")
            logging.info("="*80 + "\n")
            
            if save_comparison:
                comparison_name = f"comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                save_dir = Path("results")
                save_dir.mkdir(parents=True, exist_ok=True)
                
                comparison_path = save_dir / f"{comparison_name}.json"
                safe_json_dump(comparison, comparison_path)
                logging.info(f"Comparación guardada en: {comparison_path}")
                
                if 'performance_improvement' in comparison:
                    comparison_df = pd.DataFrame([comparison['performance_improvement']])
                    csv_path = save_dir / f"{comparison_name}.csv"
                    comparison_df.to_csv(csv_path, index=False)
                    logging.info(f"CSV de mejoras guardado en: {csv_path}")
            
            return comparison
            
        except Exception as e:
            logging.error(f"Error comparando resultados: {e}")
            raise EmbeddingsPipelineError(f"Error en comparación de resultados: {e}") from e
    
    def __str__(self) -> str:
        """
        Representación en cadena del pipeline.

        Returns:
            str: Representación en cadena del pipeline.
        """
        return f"EmbeddingsPipeline(experiment={self.experiment_name})"
    
    def __repr__(self) -> str:
        """
        Representación detallada del pipeline.
        Returns:
            str: Representación detallada del pipeline.
        """
        return (
            f"EmbeddingsPipeline("
            f"experiment={self.experiment_name}, "
            f"dataset={'✓' if self.dataset_dict else '✗'}, "
            f"baseline={'✓' if self.baseline_embeddings is not None else '✗'}, "
            f"finetuned={'✓' if self.finetuned_embeddings is not None else '✗'})"
        )


# Factory function
def create_embeddings_pipeline(
    config: Dict[str, Any],
    df: pd.DataFrame,
    experiment_name: Optional[str] = None
) -> EmbeddingsPipeline:
    """
    Factory function to create an EmbeddingsPipeline instance.

    Args:
        config (Dict[str, Any]): Configuration dictionary for the pipeline.
        df (pd.DataFrame): DataFrame containing the data.
        experiment_name (Optional[str], optional): Name of the experiment. Defaults to None.

    Returns:
        EmbeddingsPipeline: An instance of EmbeddingsPipeline.
    """
    return EmbeddingsPipeline(config, df, experiment_name)